---
title: 线性回归和梯度下降
date: 2016-06-04 23:20:31
mathjax: true
toc: true
tags:
- machine learing
- math
- algorithm
---

找了工作之后好像好久没有更新了,很多东西都忘记了,刚好最近正在重新入门machine learning,就写一下最简单的线性回归,重新捡一下快忘记的numpy, matplotlib,和latex.

### 准备
现在假设我们有$m$个数据$x$,并且每个数据都有$n$个特征,相应的这些数据所对应的结果为$y$,也是$m$个.我们希望能找到一组线性参数来对任意一个样本尽量准的预测其结果.这样的参数向量我们称为$\theta$,而我们希望用这样一个函数来预测$y$
$$
y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_n x_n
$$
或者用简洁的矩阵表示
$$
y = \theta^Tx
$$
而梯度下降法则是提供了一个有效的学习方法,从每次训练得到的反馈中修正参数直到收敛.

那根据上面的定义,我们就能很明确在这个过程中有两件事很关键.

**一是如何度量预测的准不准**

**二是如何根据每次的反馈修正参数**

### 度量预测

通过$\theta$和$x$,我们可以预测出对应的$y$,然而这个预测出的$y$和真实的$y$肯定是有误差的,那么自然而然,我们在借用统计里的方法,可以定义这个准确程度为
$$
J(\theta)=\sum_{i=1}^m \left( h_{\theta}(x^{i}) - y^{(i)} \right)^2
$$
现在我们有了一个可以度量误差的方法,那么,我们的目标也就显而易见了,就是使这个误差函数最小.而如何使其最小则是梯度下降法的工作了.

当我们得到一个函数,我们怎么能够知道往哪个方向走下一步能够最快的接近最小值呢？对高等数学熟悉的话一下子就能想到梯度的概念---在函数上的任意一点,其导数的方向就是函数值变化最大的方向.那么我们可以通过求导来获得下一步的方向.
$$
\begin{aligned}
\frac{\partial J(\theta)}{\partial \theta_j} &=  \frac{\partial}{\partial \theta_j} \sum_{i=1}^m \left (h_{\theta}(x^{i}) - y^{(i)} \right )^2 \\
&=  2\sum_{i=1}^m \left ( (h_{\theta}(x^{i}) - y^{(i)}) \, \frac{\partial}{\partial \theta_j}(h_{\theta}(x^{i}) - y^{(i)}) \right ) \\
&= 2\sum_{i=1}^m \left( h_{\theta}(x^{i}) - y^{(i)}\right) x^{(i)}_j
\end{aligned}
$$
在很多参考书中,为了整洁美观,都会使$J(\theta)$前多加一个参数$\frac{1}{2}$,使得导数变成$\sum_{i=1}^m \left( h_{\theta}(x^{i}) - y^{(i)}\right) x_i$,我们随大流,也用这样的方法定义,不过其实都没有差啦.


由此我们得到每一次的迭代方法
$$
\theta_i := \theta_i - \alpha\sum_{i=1}^m \left( h_{\theta}(x^{i}) - y^{(i)}\right) x^{(i)}_j \tag{ for i=1,2, ... ,n }
$$

这里的$\alpha$是每次梯度下降的步长,步长如果选短了会下降的很慢,迭代很多次,二步长选长了则会出现在最优点附近徘徊甚至直接远离最优点.选好$\alpha$真是一门学问.

通过这种方法迭代的梯度下降法称为批量梯度爱心将法,因为每一次迭代都需要所有的训练点参与计算.二还有另外一种增量梯度下降算法或者说最忌梯度下降算法则每一次只使用一个点来训练,他的迭代公式是
$$ 
\theta := \theta - \alpha \left( \theta^T x^{(i)} - y^{(i)} \right)x^{(i)}_j 
$$

### 我有图片

好的,下面我们就要上图了.

下面三个图都是迭代次数为10的情况，左上角是批量梯度下降，右上角是随机梯度下降，下方是在等高线上参数的步进情况。
$$ 
\alpha = 0.5
$$
![gridient_1.png](https://raw.githubusercontent.com/bates-hu/Images/master/blog/gridient_1.png)
$$ 
\alpha = 1
$$
![gridient_2.png](https://raw.githubusercontent.com/bates-hu/Images/master/blog/gridient_2.png)
$$ 
\alpha = 1.5
$$
![gridient_3.png](https://raw.githubusercontent.com/bates-hu/Images/master/blog/gridient_3.png)


### 最小二乘法的概率解释

我们在前面只用了一些特征来对这个模型进行剑魔，而在现实中相对所有的影响因素剑魔是不太可能的，所以在预测过程中肯定会有很多未捕捉或者说未参与建模的特征没有在函数中体现，我们可以利用一个误差项  $\epsilon$  来统一的表示这个误差。
    那么我们的预测模型就可以这样写
$$ 
y^{(i)} = h_{\theta}(x^{(i)}) + \epsilon^{(i)} 
$$
而现在我们就为了解释这个模型提出一个假设，我们假设误差满足高斯分布
$$
\epsilon \sim \mathscr{N}(0, \sigma^2)
$$
即
$$ 
P(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2}) 
$$

为什么我们能够先验的认为这个误差的分布满足正态分布呢？首先，也是最重要的，时间表明这个方法对于大多数情况都描述的很好，更理论一些的要涉及到概率论的中心极限定理，如果有概率论基础的自然明白，没有的话可以直接视为正态分布是对随机误差的非常优秀的刻画。


根据$\epsilon$的概率分布，我们可以得到$x$,$y$的概率分布。
$$ 
P(y^{(i)}|x^{(i)};\theta) =\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)} - h_\theta(x^{(i)}))^2}{2\sigma^2}) 
$$
即
$$ 
y^{(i)}  \sim  \mathscr{N}( h_\theta (x^{(i)}), \sigma^2)
$$

如此，我们就可以对$\theta$进行参数估计，当当当，极大似然估计登场。我们用极大似然估计的方法给出$\theta$的估计值，即我们队$\theta$的估计值就是使下式值最大的$\theta$
$$ 
L(\theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{(y^{(i)} - h_\theta(x^{(i)}))^2}{2\sigma^2}\right)
$$
不改变单调性，我们通常取对数来处理
$$
\begin{aligned}
l(\theta) = ln\left(L(\theta)\right) &= \sum_{i=1}^n ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right)-\frac{(y^{(i)} - h_\theta(x^{(i)}))^2}{2\sigma^2} \\
&= mln\left(\frac{1}{\sqrt{2\pi}\sigma}\right) + \frac{1}{2\sigma^2}\sum_{i=1}^n -\left( y^{(i)}-h_\theta(x^{(i)}) \right)^2
\end{aligned}
$$

因为$\sigma$是常数，那么我们只需求使
$$ 
\sum_{i=1}^n \left( y^{(i)}-h_\theta(x^{(i)}) \right)^2 
$$
最小就达到了最大似然估计的要求。

是不是很眼熟，这个就是我们一开始定义的对误差的度量。

至此，我们就得到在先前得到的描述误差的函数，表示这个误差描述函数不是凭空掉下的林妹妹~


